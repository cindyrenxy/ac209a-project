{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8773fc3-fec8-4162-869d-17256741dda2",
   "metadata": {},
   "source": [
    "# 1. Problem Overview\n",
    "\n",
    "Predicting patient outcomes in the intensive care unit (ICU) is a critical challenge. Early identification of patients at high risk of mortality enables clinicians to prioritize interventions, allocate resources efficiently, and improve overall patient care. Machine learning and statistical modeling approaches can serve as powerful tools for improving outcome prediction as they can capture complex relationships and patterns inherent in patient data. \n",
    "\n",
    "However, due to the high dimensionality, missingness, and correlations present in ICU datasets, it is critical to develop models that balance predictive performance with interpretability and clinical practicality. Logistic regression models offer simple and interpretable quantifications of the associations between predictors and outcomes, whereas random forests can model complex, nonlinear relationships and interactions between variables.\n",
    "\n",
    "By applying both of these methods to ICU data collected within the first 24 hours of admission, this project aims to identify key predictors of patient mortality and construct a predictive model. Through evaluation metrics such as area under the curve (AUC) and precision and recall, we will demonstrate the feasibility of implementing our model in real-world ICU settings to support timely and evidence-based decision making. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be5d8b9-0ad7-4328-9422-7c8523560054",
   "metadata": {},
   "source": [
    "# 2. Finalized Research Question\n",
    "How can logistic regression and random forest models be used to predict the risk of patient mortality in the intensive care unit (ICU) within the first 24 hours of admission?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad67fdb2-47b4-4ba2-ac1f-672b11923f34",
   "metadata": {},
   "source": [
    "# 3. Data Overview\n",
    "\n",
    "This project utilizes a publicly available dataset collected by MIT’s Global Open Source Severity of Illness Score (GOSSIS) community initiative. The dataset includes comprehensive clinical information from over 130,000 ICU visits recorded over the course of one year. Data were aggregated from more than 200 hospitals across multiple countries, including the United States, Argentina, Australia, New Zealand, Sri Lanka, and Brazil, reflecting a diverse and globally representative ICU population. The dataset’s target variable is hospital mortality, while the predictor variables include a wide range of clinically relevant features collected within the first 24 hours of ICU admission.\n",
    "\n",
    "The datasets used for model training, selection, and evaluation in this project include:\n",
    "- `training_v2.csv`: Contains 91,713 encounters along with associated clinical factors for model training.\n",
    "- `unlabeled.csv`: Contains the same clinical features but lacks the `hospital_death` outcome; this dataset will be used for generating predictions.\n",
    "- `WiDS Datathon 2020 Dictionary.csv`: Provides supplemental information on the dataset, including detailed descriptions of the predictors and clinical factors.\n",
    "\n",
    "The response variable for this project is `hospital_death`, a binary outcome where 0 indicates the patient survived and 1 indicates the patient deceased. The remaining 185 columns serve as potential predictors. The objective is to perform feature selection to reduce the model to ≤10 non-collinear predictors, and then apply the simplified model to generate predictions on the unlabeled.csv dataset.\n",
    "\n",
    "**Note**: Some columns may not be suitable as potential predictors. For example, `encounter_id` is unlikely to be informative for predicting `hospital_death`. The team will conduct a thorough manual review to identify and remove any columns that are not appropriate for inclusion as predictors.\n",
    "\n",
    "**Note**: There are several ethical considerations we will have to keep in mind while using this dataset. For instance, clinical datasets of this nature contain highly sensitive and private information, so the dataset should remain de-identified and comply with ethical standards when reusing it. Also, differences in healthcare quality and resource availability across hospitals from diverse countries could introduce systemic bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c00385-37a0-467b-8d5d-646f05e3cb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf48faf2-c8e5-4622-b7d8-8b341fef60e8",
   "metadata": {},
   "source": [
    "## 3.1 Summary of Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9201b9e1-6589-4410-9040-2c1d60863c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/training_v2.csv\") # load the dataset\n",
    "df.head() # exploratory summary of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3e096d-5a5e-4e85-9802-2e3bcd952abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Shape of dataset: {df.shape}\\n\")\n",
    "print(\"Column data types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nDescriptive statistics:\")\n",
    "display(df.describe(include='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56609e87-2aa5-4d01-a9e5-200b6deb348b",
   "metadata": {},
   "source": [
    "## 3.2 Histogram\n",
    "Since there are 186 columns, we only selected a few among the numeric columns that might play a role as a significant predictor to gain an overview of their distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8720fbe4-8e5a-4d4d-8e9b-26ac901c3f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_selection = ['age', 'albumin_apache', 'bun_apache', 'glucose_apache', 'heart_rate_apache', 'map_apache', 'd1_temp_max', 'd1_resprate_max', 'h1_wbc_max']\n",
    "df[initial_selection].hist(figsize=(15, 12), bins=30, edgecolor='black')\n",
    "plt.suptitle(\"Distribution of Numeric Features\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd58ba5-b912-431e-90d0-642dd6f2b837",
   "metadata": {},
   "source": [
    "# 4. Data Issues\n",
    "\n",
    "## 4.1 Data Missingness \n",
    "We first examined the missingness in our dataset. Excluding the response variable `hospital_death`, the dataset contains a total of 185 potential predictors. We performed a preliminary check on the percentage of missing values in each predictor and observed the following:\n",
    "- 70 predictors have missingness ≤ 5%, where missing data can be imputed relatively confidently using standard imputation methods.\n",
    "- 33 predictors have missingness between 5% and 20%.\n",
    "- 82 predictors have missingness > 20%, making imputation more challenging and potentially less reliable.\n",
    "\n",
    "We also examined data missingness in survived (`'hospital_death' == 0`) and deceased (`'hospital_death' == 1`) populations. The bars for both groups largely overlap at low missingness, suggesting missingness is fairly similar across the two groups for most predictors. At higher missingness, there is a slight difference, but overall it seems the missingness pattern is not drastically different between the groups.\n",
    "\n",
    "Based on a review of the literature, we decided to select a cutoff value of [???]% to mitigate potential bias that may arise from handling missing data through imputation or alternative methods, depending on the nature of each predictor. The team will further discuss the most appropriate strategy for addressing missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8741e5-b09a-4cc1-84fe-5bd9efadc38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = plt.cm.Blues(np.linspace(0.45, 0.9, 10))\n",
    "na_percent = df.drop(columns=['hospital_death'], errors='ignore').isna().mean()*100\n",
    "ax = (na_percent.sort_values(ascending=False)\n",
    "      .head(10)\n",
    "      .plot(kind='barh', figsize=(6,5), color=colors))\n",
    "ax.invert_yaxis()\n",
    "ax.set_title(\"Top 10 Features by Missingness (%)\")\n",
    "ax.set_xlabel(\"%\")\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd41839f-3478-4724-97a8-276ff26c452b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()\n",
    "\n",
    "#   ======== EXAMINE MISSINGNESS IN NUMERICAL COLUMNS ========\n",
    "# calculate persontage of missingness for each column\n",
    "na_percent = df.isna().mean() * 100\n",
    "na_percent = na_percent.drop('hospital_death') # drop response variable\n",
    "na_percent = na_percent.sort_values(ascending=True)\n",
    "\n",
    "# visualize percentage of missingness\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(8, 10))\n",
    "sns.histplot(na_percent, bins=30, ax=ax1)\n",
    "ax1.axvline(x=5, color='k', linestyle='--', label='5% Missingness Threshold')\n",
    "ax1.axvline(x=20, color='red', linestyle='--', label='20% Missingness Threshold')\n",
    "ax1.set_title(\"Histogram of Missingness Percentage Across Predictors\")\n",
    "ax1.legend()\n",
    "\n",
    "# display stats\n",
    "print(f'Number of predictors with <= 5% missingness: {sum(na_percent <= 5)}')\n",
    "print(f'Number of predictors with 5-20% missingness: {sum((na_percent > 5) & (na_percent <= 20))}')\n",
    "print(f'Number of predictors with > 20% missingness: {sum(na_percent > 20)}')\n",
    "\n",
    "# calculate % missingness in response\n",
    "print(f\"Percentage of missingness in hospital_death: {df['hospital_death'].isna().mean() * 100}\")\n",
    "\n",
    "#   ======== EXAMINE MISSINGNESS IN CATEGORICAL COLUMNS ========\n",
    "# select only categorical/object columns\n",
    "cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# calculate missing counts & percentages for categorical columns\n",
    "cat_na_count = df[cat_cols].isna().sum()\n",
    "cat_na_percent = df[cat_cols].isna().mean() * 100\n",
    "cat_na_percent = cat_na_percent.sort_values(ascending=True)\n",
    "\n",
    "# visualize missingness\n",
    "sns.histplot(cat_na_percent, bins=20, ax=ax2)\n",
    "ax2.axvline(x=5, color='k', linestyle='--', label='5% Missingness Threshold')\n",
    "ax2.axvline(x=20, color='red', linestyle='--', label='20% Missingness Threshold')\n",
    "ax2.set_title(\"Histogram of Missingness Percentage Across Categorical Predictors\")\n",
    "ax2.set_xlabel(\"Percentage Missing\")\n",
    "ax2.legend()\n",
    "fig.tight_layout()\n",
    "\n",
    "# display stats\n",
    "print(f'Number of categorical predictors with <= 5% missingness: {sum(cat_na_percent <= 5)}')\n",
    "print(f'Number of categorical predictors with 5-20% missingness: {sum((cat_na_percent > 5) & (cat_na_percent <= 20))}')\n",
    "print(f'Number of categorical predictors with > 20% missingness: {sum(cat_na_percent > 20)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ff10a6-144a-4a6e-804d-927958f690fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize percentage of missingness by response variable\n",
    "df_survived = df[df['hospital_death'] == 0]\n",
    "df_deceased = df[df['hospital_death'] == 1]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "sns.histplot(df_survived.isna().mean() * 100, bins=30, ax=ax, color='blue', alpha=0.5, label='Survived')\n",
    "sns.histplot(df_deceased.isna().mean() * 100, bins=30, ax=ax, color='orange', alpha=0.5, label='Deceased')\n",
    "ax.axvline(x=5, color='k', linestyle='--', label='5% Missingness Threshold')\n",
    "ax.axvline(x=20, color='red', linestyle='--', label='20% Missingness Threshold')\n",
    "ax.set_title(\"Histogram of Missingness Percentage Across Predictors\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4af372b-d1a1-4287-b106-66044bf2a480",
   "metadata": {},
   "source": [
    "## 4.2 Data Imbalance\n",
    "We examined the class distribution of our target variable `hospital_death`, and found a significant imbalance: 83,798 patients survived, while 7,915 patients did not. Due to this pronounced imbalance, we decided to incorporate resampling techniques during model training to mitigate its impact and improve the model’s performance on the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00099836-500b-4cb0-9015-19d22787f890",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['hospital_death'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1984b204-1b7d-4465-9b63-7a8fe0b9ce4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "imbalance_ratio = df['hospital_death'].value_counts(normalize=True)\n",
    "print(imbalance_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06335a45-eb63-4495-9045-b0b6e789a82c",
   "metadata": {},
   "source": [
    "## 4.3 Data Scaling\n",
    "We began by removing all ID-related columns to ensure that identifier values were not treated as quantitative predictors. After doing so, we observed that most features have very small ranges, clustered near zero, while a few features have extremely large ranges (e.g., one reaching ~80,000). This is a classic case of features on widely different scales, meaning some features could dominate model training if not scaled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fa56e9-6228-4772-8407-e837186670c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns.drop(['encounter_id', 'patient_id', 'hospital_id', 'icu_id'])\n",
    "df[numeric_cols].describe() # exploratory summary\n",
    "\n",
    "# visualize feature ranges\n",
    "ranges = df[numeric_cols].max() - df[numeric_cols].min()\n",
    "fig, ax = plt.subplots()\n",
    "sns.histplot(ranges, bins=30, edgecolor='k')\n",
    "ax.set_title(\"Histogram of Feature Ranges\")\n",
    "ax.set_xlabel(\"Range\")\n",
    "ax.set_ylabel(\"Number of Features\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1fda90-044b-4c3e-88d6-73024c88eec3",
   "metadata": {},
   "source": [
    "After removing ID-related variables, we examined the distribution of feature ranges to identify differences in scale across numerical predictors. The plot below shows the top 10 features with the largest value ranges.\n",
    "\n",
    "Key observations:\n",
    "\n",
    "- **`urineoutput_apache`** has the widest range. According to the data dictionary, this represents urine output prior to ICU admission (used in the APACHE score). Urine output varies widely in critically ill patients, from anuric kidney failure to high-output states, making a large range clinically plausible.\n",
    "\n",
    "- **`apache_3j_diagnosis`** also shows a large range. This feature represents APACHE diagnostic categories encoded as numeric values. Although it is stored as a number, it functions as a *categorical* variable rather than a continuous physiologic measurement. Its wide numerical spread reflects category codes, not quantitative magnitude. This suggests that additional preprocessing is needed to ensure these encoded values are treated as categorical labels rather than continuous numeric inputs, and to prevent string-like numeric codes without quantitative meaning from influencing model training.\n",
    "\n",
    "- Several physiologic variables also appear among the largest-range features, including:\n",
    "  - **PaO₂/FiO₂ ratio** (`d1_pao2fio2ratio_max`, `h1_pao2fio2ratio_max`, `d1_pao2fio2ratio_min`, `h1_pao2fio2ratio_min`), a key indicator of oxygenation\n",
    "  - **Glucose** (`h1_glucose_max`, `h1_glucose_min`), reflecting metabolic disturbances common in ICU patients\n",
    "  - **Platelets** (`h1_platelets_min`, `h1_platelets_max`), which can vary due to bleeding, sepsis, or bone marrow dysfunction\n",
    "\n",
    "These findings highlight substantial variation in feature scales across the dataset. Without proper preprocessing, high-range features may dominate model training. This reinforces the need for feature scaling (e.g., standardization or normalization) to ensure balanced contribution from all variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b9c218-9f2f-4845-93ac-3a7df0d0f6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort and display\n",
    "range_sorted = ranges.sort_values(ascending=False)\n",
    "print(range_sorted.head(10))\n",
    "\n",
    "# plot top 10 features with largest ranges\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=range_sorted.head(10).values, y=range_sorted.head(10).index)\n",
    "plt.title(\"Top 10 Features With Largest Ranges (Without patient_id)\")\n",
    "plt.xlabel(\"Range\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e45277-3578-40eb-b9d3-ff2453b2debf",
   "metadata": {},
   "source": [
    "We also want to examine whether categorical variables exist and how to encode them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a605037-eae8-4d9f-b9d0-91ba98d6f980",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns\n",
    "print(\"Categorical columns:\", list(cat_cols))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb02c36-d31b-4736-9323-4520ba787384",
   "metadata": {},
   "source": [
    "Among our categorical variables, we will retain and encode the following features:\n",
    "- `ethnicity`  \n",
    "- `gender`  \n",
    "- `apache_3j_bodysystem`  \n",
    "- `apache_2_bodysystem`  \n",
    "We exclude variables such as `hospital_admit_source`, `icu_admit_source`, `icu_stay_type`, and `icu_type`, as they  primarily capture administrative or operational details and are unlikely to have a direct causal relationship with mortality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75097a30-9a01-4126-af44-744fe74f896e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = ['ethnicity', 'gender', 'apache_3j_bodysystem', 'apache_2_bodysystem']\n",
    "df_encoded = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n",
    "print(\"Original shape:\", df.shape)\n",
    "print(\"Encoded shape:\", df_encoded.shape)\n",
    "\n",
    "encoded_cols = [col for col in df_encoded.columns if any(base in col for base in cat_cols)]\n",
    "print(\"Example of encoded columns:\", encoded_cols[:10])\n",
    "df_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbaa822-c7cf-450b-8a4c-f83e058c64fc",
   "metadata": {},
   "source": [
    "After one-hot encoding, the dataset expanded from 186 to 207 columns, reflecting the creation of binary indicator variables for the selected categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4113e90c-c487-4377-ab41-cd3a9b4e2ab4",
   "metadata": {},
   "source": [
    "## 4.4 Correlation Check\n",
    "We examined pairwise correlations among numeric predictors to identify potential multicollinearity that could affect interpretability. The heatmap below shows absolute Pearson correlations among the first 15 numeric features with less than 20% missingness (excluding IDs and the label). Some variables show strong positive correlations, indicating redundancy that will be addressed during feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6f347c-4dac-46fd-8b68-f1b23c654a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_df = (df.select_dtypes(include=['float64', 'int64'])\n",
    "              .drop(columns=['hospital_death', 'encounter_id', 'patient_id', 'hospital_id', 'icu_id'], errors='ignore'))\n",
    "\n",
    "low_missing_cols = num_df.columns[num_df.isna().mean() < 0.20].tolist()\n",
    "\n",
    "corr_matrix_low = num_df[low_missing_cols].corr().abs()\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix_low.iloc[:15, :15], cmap='coolwarm', center=0, linewidths=0.5)\n",
    "plt.title(\"Correlation Heatmap (Numeric Features with <20% Missingness)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Used {len(low_missing_cols)} / {num_df.shape[1]} numeric features (<20% missingness).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6345efac",
   "metadata": {},
   "source": [
    "# 5. EDA\n",
    "From this point forward, we will work with the original dataset. To ensure consistency and reproducibility, we will reload both the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52118586",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/training_v2.csv\") # load the dataset\n",
    "df_test = pd.read_csv(\"data/unlabeled.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c7e799",
   "metadata": {},
   "source": [
    "## 5.1 Feature Selection\n",
    "Based on our prior analysis of missingness in the dataset, we have decided to focus our modeling efforts on a subset of features that meet defined completeness criteria. \n",
    "\n",
    "First, all ID-related columns will be removed. Specifically, any column in the` WiDS Datathon 2020 Dictionary`.csv labeled with a description of \"identifier\" will be dropped. This results in a drop of 4 columns. \n",
    "\n",
    "We will then focus our analysis on the 100 numeric features with less than 30% missingness. After including the binary response variable, `hospital_mortality`, this yields a total of 101 numeric columns (including id columns, which are removed at a later step) to retain for modeling.\n",
    "\n",
    "For categorical variables, we will retain only those columns with a missingness rate below 5%, ensuring high data quality and minimizing potential biases that could arise from imputation or sparse categories. This results in a total of 7 categorical columns. \n",
    "\n",
    "For the purposes of predicting mortality, we will also include certain administrative-related variables as part of our predictor set. While these columns, such as `icu_admit_source`, `hospital_admit_source`, `icu_stay_type`, and `icu_type`, are primarily operational in nature, they may carry predictive value for mortality outcomes. For example, patients admitted from specific ICU sources or transfer pathways may inherently have a higher risk of adverse outcomes. Therefore, despite their administrative context, these variables will be incorporated into our predictive models to potentially improve performance, while recognizing that their inclusion is for prediction rather than causal inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b8eb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define id columns to drop\n",
    "id_cols = ['encounter_id', 'hospital_id', 'patient_id', 'icu_id']\n",
    "print(f\"ID columns to drop ({len(id_cols)} columns):\")\n",
    "print(id_cols)\n",
    "\n",
    "# keep numeric columns with <30% missingness\n",
    "num_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "# calculate missing counts & percentages for categorical columns\n",
    "num_na_count = df[num_cols].isna().sum()\n",
    "num_na_percent = df[num_cols].isna().mean() * 100\n",
    "# num_na_percent = num_na_percent.sort_values(ascending=True)\n",
    "\n",
    "num_cols_lt30 = num_na_percent[num_na_percent < 30].index.tolist()\n",
    "num_cols_lt30 = [col for col in num_cols_lt30 if col not in id_cols] # remove id columns\n",
    "num_cols_lt30 = [col for col in num_cols_lt30 if col != \"hospital_death\"] # remove response columns\n",
    "print(f\"\\nNumeric columns with <30% missingness ({len(num_cols_lt30)} columns):\")\n",
    "print(num_cols_lt30)\n",
    "\n",
    "# keep categorical columns with <5% missingness\n",
    "cat_cols_lt5 = cat_na_percent[cat_na_percent < 5].index.tolist()\n",
    "print(f\"\\nCategorical columns with <5% missingness ({len(cat_cols_lt5)} columns):\")\n",
    "print(cat_cols_lt5)\n",
    "\n",
    "# drop columns not meeting criteria\n",
    "cols_to_keep = num_cols_lt30 + cat_cols_lt5 + ['hospital_death']\n",
    "df = df[cols_to_keep]\n",
    "print(f\"\\nShape of dataset after feature selection (including both predictors and response): {df.shape}\")\n",
    "\n",
    "df_test = df_test[cols_to_keep]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7b72d7",
   "metadata": {},
   "source": [
    "## 5.2 Imputation\n",
    "### 5.2.1 Benchmarking Imputation Methods\n",
    "After performing feature selection using a 30% missingness threshold for numerical columns and 5% for categorical columns, we proceeded to evaluate several imputation strategies. For numerical features, we tested `SimpleImputer (mean and median)` and `KNNImputer`. For categorical features, we compared `SimpleImputer with the most frequent category` and `SimpleImputer with a constant “Missing” label`.\n",
    "\n",
    "To assess performance, we used __RMSE for numerical imputation__ and __accuracy (percentage of correctly recovered values)__ for categorical imputation. To ensure a clean evaluation without interference from originally missing values, we restricted our benchmarking to rows that were fully observed, resulting in 34,494 complete rows. We then simulated missingness by masking 20% of the values and applied each imputation method to recover the masked entries. The reconstructed values were compared against the true values to quantify imputation performance.\n",
    "\n",
    "__Note__: The benchmarking cell is commented due to long run time. Best model is saved below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f965985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify response variable\n",
    "target = \"hospital_death\"\n",
    "y = df[target]\n",
    "X = df.drop(columns=[target])\n",
    "X_unlabeled = df_test\n",
    "\n",
    "# list of predictor columns\n",
    "num_cols = num_cols_lt30\n",
    "cat_cols = cat_cols_lt5\n",
    "df[cat_cols] = df[cat_cols].astype(\"category\") # convert categorical columns to \"category\" type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c8e8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa: F401\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76608092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def make_random_mask(df_nona, missing_rate=0.2, seed=42):\n",
    "#     rng = np.random.default_rng(seed)\n",
    "#     mask = pd.DataFrame(\n",
    "#         rng.random(df_nona.shape) < missing_rate,\n",
    "#         index=df_nona.index,\n",
    "#         columns=df_nona.columns)\n",
    "#     return mask\n",
    "\n",
    "# def impute(df_masked, num_cols, cat_cols, num_method, cat_method):\n",
    "#     df_imputed = df_masked.copy()\n",
    "#     # impute numerical columns\n",
    "#     num_vals = num_method.fit_transform(df_imputed[num_cols])\n",
    "#     df_imputed[num_cols] = num_vals\n",
    "\n",
    "#     # inpute categorical columns\n",
    "#     cat_vals = cat_method.fit_transform(df_imputed[cat_cols])\n",
    "#     df_imputed[cat_cols] = np.array(cat_vals).reshape(-1, len(cat_cols))\n",
    "\n",
    "#     return df_imputed\n",
    "\n",
    "# # evaluate performance of \n",
    "# def evaluate_imputation(df_nona, df_imputed, mask, num_cols, cat_cols):\n",
    "#     # evaluate numerical cols imputations using RMSE\n",
    "#     all_num = df_nona[num_cols]\n",
    "#     imp_num = df_imputed[num_cols]\n",
    "#     specific_num_mask = mask[num_cols]\n",
    "\n",
    "#     # extract only the values where the mask is True (the imputed positions)\n",
    "#     true_num_values = all_num.values[specific_num_mask.values]\n",
    "#     imp_num_values = imp_num.values[specific_num_mask.values]\n",
    "\n",
    "#     # calculate RMSE\n",
    "#     if len(true_num_values) > 0:\n",
    "#         rmse = np.sqrt(mean_squared_error(true_num_values, imp_num_values))\n",
    "#     else:\n",
    "#         rmse = 0 # handle case where no values were imputed\n",
    "\n",
    "#     # evaluate categorical cols imputations using accuracy\n",
    "#     cat_cols_to_evaluate = df_nona[cat_cols]\n",
    "#     imp_cat_cols = df_imputed[cat_cols]\n",
    "#     specific_cat_mask = mask[cat_cols]\n",
    "\n",
    "#     true_cat_values = cat_cols_to_evaluate.values[specific_cat_mask.values]\n",
    "#     imp_cat_values = imp_cat_cols.values[specific_cat_mask.values]\n",
    "\n",
    "#     if len(true_cat_values) > 0:\n",
    "#         acc = (true_cat_values == imp_cat_values).mean()\n",
    "#     else:\n",
    "#         acc = 1.0 # Or 0, depending on context\n",
    "    \n",
    "#     f1_macro = f1_score(true_cat_values, imp_cat_values, average=\"macro\")\n",
    "\n",
    "#     return rmse, acc, f1_macro\n",
    "\n",
    "# # specify imputers to benchmark\n",
    "# num_imputers = {\n",
    "#   \"mean\": SimpleImputer(strategy=\"mean\"),\n",
    "#   \"median\": SimpleImputer(strategy=\"median\"),\n",
    "#   \"knn\": KNNImputer(n_neighbors=5),\n",
    "#   \"mice\": IterativeImputer(\n",
    "#         max_iter=10,\n",
    "#         random_state=42,\n",
    "#         sample_posterior=True\n",
    "#     ),\n",
    "# }\n",
    "\n",
    "# cat_imputers = {\n",
    "#     \"most_frequent\": SimpleImputer(strategy=\"most_frequent\"),\n",
    "#     \"constant_missing\": SimpleImputer(strategy=\"constant\", fill_value=\"Missing\")\n",
    "# }\n",
    "\n",
    "# # use rows without NAs to evaluate performances of different imputers\n",
    "# df_nona = X.copy().dropna().reset_index(drop=True) # copy non NA rows for imputation becnhmarking\n",
    "\n",
    "# mask = make_random_mask(df_nona, missing_rate=0.2)\n",
    "# df_masked = df_nona.mask(mask)\n",
    "\n",
    "# results = [] # initialize a list to store result\n",
    "\n",
    "# for num_name, num_imp in num_imputers.items():\n",
    "#     for cat_name, cat_imp in cat_imputers.items():\n",
    "#         df_imputed = impute(df_masked, num_cols, cat_cols, num_imp, cat_imp)\n",
    "#         rmse, acc, f1_macro = evaluate_imputation(df_nona, df_imputed, mask, num_cols, cat_cols)\n",
    "#         results.append({\"num_method\": num_name, \"cat_method\": cat_name, \"RMSE\": rmse, \"Accuracy\": acc, \"F1_Macro\": f1_macro})\n",
    "\n",
    "# results = pd.DataFrame(results)\n",
    "# results.sort_values(by=[\"RMSE\", \"Accuracy\"], ascending=[True, False], inplace=True)\n",
    "# results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37fe439",
   "metadata": {},
   "source": [
    "### 5.2.2 Choice of Imputers for Numerical and Categorical Columns\n",
    "Based on the benchmarking results, the `median imputer` achieved the best performance for numerical columns, while the` most frequent imputer` performed best for categorical columns. Using these findings, we constructed a preprocessing pipeline that applies __median imputation and scaling__ to numerical features, and __most-frequent imputation followed by one-hot encoding__ to categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb4c770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transformers\n",
    "num_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "    (\"scaler\", StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer([\n",
    "    (\"num\", num_pipe, num_cols),\n",
    "    (\"cat\", cat_pipe, cat_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f01039a",
   "metadata": {},
   "source": [
    "## 5.3 Principle Component Analysis\n",
    "After data imputation and preprocessing, we applied Principal Component Analysis to better understand the intrinsic structure of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749e7698",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_processed = preprocess.fit_transform(X)\n",
    "if hasattr(X_processed, \"toarray\"):\n",
    "    X_dense = X_processed.toarray()\n",
    "else:\n",
    "    X_dense = X_processed\n",
    "\n",
    "feature_names = preprocess.get_feature_names_out()\n",
    "X_processed_df = pd.DataFrame(X_dense, columns=feature_names)\n",
    "X_processed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5033ba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca_full = PCA().fit(X_processed)\n",
    "explained = pca_full.explained_variance_ratio_\n",
    "print(\"First 10 PCs:\", explained[:10])\n",
    "print(\"Cumulative variance (first 10):\", explained[:10].sum())\n",
    "cumulative_var = np.cumsum(explained)\n",
    "num_components_90 = np.argmax(cumulative_var >= 0.90) + 1\n",
    "print(f\"Number of components needed for 90% variance: {num_components_90}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9f7ef0",
   "metadata": {},
   "source": [
    "We found that the first 46 principal components were required to explain 90% of the variance, indicating that the ICU dataset is highly heterogeneous and cannot be captured in only a few dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75430e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(cumulative_var, marker='o')\n",
    "plt.axhline(0.90, color='red', linestyle='--')\n",
    "plt.xlabel(\"Number of PCA Components\")\n",
    "plt.ylabel(\"Cumulative Explained Variance\")\n",
    "plt.title(\"Cumulative Explained Variance of PCA Components\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e482e460",
   "metadata": {},
   "source": [
    "For visualization, we projected the data onto the first two principal components which explain about 20 percent of total variance. The scatter plot shows substantial overlap between survivors and non-survivors, suggesting that hospital mortality is not linearly separable in the most dominant directions of variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b067f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_2 = PCA(n_components=2)\n",
    "X_pca_2 = pca_2.fit_transform(X_processed)\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(X_pca_2[:,0], X_pca_2[:,1], c=y, cmap=\"coolwarm\", alpha=0.6)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"PCA Projection (PC1 vs PC2)\")\n",
    "plt.colorbar(label=\"hospital_death\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924431b7",
   "metadata": {},
   "source": [
    "## 5.4 K-means Clustering\n",
    "To further explore potential patient subgroups, we applied KMeans clustering to the PCA-reduced data. Because the first 46 principal components captured approximately 90% of the total variance, we performed KMeans in the 46-dimensional PCA space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd13444e",
   "metadata": {},
   "source": [
    "To evaluate cluster quality, we computed Silhouette scores on a 5000 sample subset of the PCA-46 representation. The highest Silhouette score occurred at k = 2 and consistently declined as k increased. This indicates that the dataset does not contain naturally well-separated high-order clusters, and increasing the number of clusters beyond three leads to over-partitioning rather than discovering meaningfully distinct groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea84834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "pca_46 = PCA(n_components=46)\n",
    "X_pca_46 = pca_46.fit_transform(X_processed)\n",
    "\n",
    "sample_idx = np.random.choice(len(X_pca_46), size=5000, replace=False)\n",
    "X_sample = X_pca_46[sample_idx]\n",
    "\n",
    "sil_scores = {}\n",
    "cluster_assignments = {}\n",
    "\n",
    "for k in range(2, 10):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    clusters_46 = kmeans.fit_predict(X_pca_46)\n",
    "    cluster_assignments[k] = clusters_46 \n",
    "    \n",
    "    clusters_sample = kmeans.predict(X_sample)\n",
    "    sil = silhouette_score(X_sample, clusters_sample)\n",
    "    sil_scores[k] = sil\n",
    "\n",
    "sil_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843f9d80",
   "metadata": {},
   "source": [
    "We then visualized the clusters in the 2D PCA space using the first two principal components. Although the clusters appear cleanly separated in the plot, PC1 and PC2 together explain only about 20% of the total variance, so this apparent separation does not reflect true geometric separability in the original high-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12af94ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "clusters_k = cluster_assignments[k]\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca_2[:, 0], X_pca_2[:, 1], c=clusters_k, cmap=\"tab10\", alpha=0.6)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.title(\"KMeans Clusters in PCA-2 Space\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8a57e5",
   "metadata": {},
   "source": [
    "Calculating the mortality rate for each cluster, we found a clear difference between the two clusters, indicating possible underlying risk stratification in the patient population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df743d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_series = pd.Series(clusters_k, index=y.index) \n",
    "mortality_by_cluster = y.groupby(cluster_series).mean()\n",
    "\n",
    "print(\"Cluster-level mortality rates:\")\n",
    "print(mortality_by_cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e85162-4f0e-4698-a47d-587e54a62be4",
   "metadata": {},
   "source": [
    "# 6. Baseline Model -- Logistic Regression Model \n",
    "As our first basic model, we used logistic regression that predicts `hospital_death` using predictors preprocessed from above.  \n",
    "\n",
    "### 6.1 Multiple Logistic Regression Model using 10-fold stratified cross-validation \n",
    "Our baseline model uses a multiple logistic regression classifier trained on preprocessed predictors to predict hospital_death. Overall, the model demonstrates strong discrimination ability, but also reveals clear limitations that motivate the need for more advanced modeling.\n",
    "\n",
    "#### Model Architecture\n",
    "We use Multiple Logistic Regression presented in the class:\n",
    "\n",
    "$$\n",
    "\\ln\\left( \\frac{P(Y = 1 \\mid X)}{1 - P(Y = 1 \\mid X)} \\right)\n",
    "= \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p .\n",
    "$$\n",
    "\n",
    "Define the linear predictor:\n",
    "\n",
    "$$\n",
    "z = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p .\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(Y = 1 \\mid X) = \\sigma(z)\n",
    "= \\frac{1}{1 + e^{-z}} .\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd205bd-50a9-483e-b4b7-5aff65f7d8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# modeling pipeline \n",
    "logreg = Pipeline([\n",
    "    (\"preprocess\", preprocess),\n",
    "    (\"model\", LogisticRegression(max_iter=1000))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4214146",
   "metadata": {},
   "source": [
    "##### Cross-Validation Performance\n",
    "\n",
    "Using 10-fold stratified cross-validation: **Mean Accuracy: ~0.925**, **Mean ROC AUC: ~0.878**\n",
    "\n",
    "These values are consistent across folds, suggesting a stable model that generalizes reasonably well. The AUC close to 0.88 indicates that the model can reliably distinguish between survivors and non-survivors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0713ba-1b99-4796-8974-79441e9c8fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "cv_results = cross_validate(\n",
    "    logreg,\n",
    "    X,\n",
    "    y,\n",
    "    cv=cv,\n",
    "    scoring=[\"accuracy\", \"roc_auc\"],\n",
    "    return_train_score=False\n",
    ")\n",
    "\n",
    "print(\"Accuracy:\", cv_results[\"test_accuracy\"].mean())\n",
    "print(\"ROC AUC:\", cv_results[\"test_roc_auc\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34aaf786-cc21-4c5a-a839-c809ee9d6a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a37661-5a71-404a-80ec-f4cdc6628662",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d120977",
   "metadata": {},
   "source": [
    "##### Performance on the Full Training Set\n",
    "\n",
    "Accuracy: 0.925\n",
    "ROC AUC: 0.879\n",
    "\n",
    "The agreement between cross-validated and full-sample AUC confirms that the model is neither highly overfitted nor underfitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0ebbd8-aec8-4221-ae74-3c886ffc3ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X)\n",
    "y_prob = logreg.predict_proba(X)[:, 1]\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y, y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y, y_prob))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a3df95-487e-4875-b920-b187a593e20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y, y_prob)\n",
    "auc_score = roc_auc_score(y, y_prob)\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {auc_score:.3f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\")  \n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941fd98d",
   "metadata": {},
   "source": [
    "##### Confusion Matrix Evaluation\n",
    "\n",
    "The confusion matrix reveals that the baseline logistic regression model performs very well on the majority class (survivors) but struggles substantially with the minority class (hospital deaths). Among the 83,798 true non-death cases, the model correctly identifies 82,680 of them, resulting in an extremely low false-positive rate. However, out of 7,915 true death cases, the model correctly predicts only 2,176, while misclassifying 5,739 deaths as survivors. This produces a recall of only 0.27 for the mortality class, indicating that the model misses a large proportion of true deaths. Although the overall accuracy appears high (driven by the overwhelming number of survivors), the imbalance in misclassification highlights a key limitation: the baseline model is not sensitive enough to detect high-risk patients, which is a critical objective in clinical settings. This motivates the need for more advanced or regularized models that can improve detection of the positive class without sacrificing overall performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da56e00b-9c4e-495b-8f35-eeec32cb028f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y, y_pred)\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa5468a",
   "metadata": {},
   "source": [
    "### 6.2  L1-regularized logistic regression (LASSO)\n",
    "For another option, we proposed to use logistic regression model using an L1 penalty (LASSO) that predicts `hospital_death` using predictors preprocessed from above. \n",
    "\n",
    "L1 regularization forces many coefficients to zero, effectively performing automatic feature selection during model training. This allows us to identify the most predictive clinical variables while avoiding overfitting in a high-dimensional dataset.\n",
    "\n",
    "#### Model Architecture\n",
    "\n",
    "The L1-penalized objective function (LASSO logistic regression) is:\n",
    "\n",
    "$$\n",
    "\\min_{\\beta} \\left[\n",
    "    -\\sum_{i=1}^{n}\n",
    "        \\left(\n",
    "            y_i \\log p_i\n",
    "            + (1 - y_i)\\log(1 - p_i)\n",
    "        \\right)\n",
    "    \\;+\\;\n",
    "    \\lambda \\sum_{j=1}^{p} |\\beta_j|\n",
    "\\right],\n",
    "$$\n",
    "\n",
    "where $\\lambda$ controls the strength of the L1 penalty.  \n",
    "Larger $\\lambda$ values produce a **sparser** model by shrinking more coefficients to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eb12a1",
   "metadata": {},
   "source": [
    "#### Data used for Training and Validation\n",
    "\n",
    "We used processed data that already gone through processes and imputations after cutoff. \n",
    "Using cross-validated tuning over a wide range of regularization strengths, the model selected a relatively small C = 0.1389, indicating that substantial regularization was beneficial for this dataset. This is consistent with the high dimensionality and multicollinearity typical of ICU clinical data, where many predictors are redundant or weakly informative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8449e5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "#Train/validation split\n",
    "X_processed = X_processed_df # use imputed dataset after cutoff\n",
    "y = df[\"hospital_death\"]\n",
    "features = X_processed.columns\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_processed,\n",
    "    y,\n",
    "    test_size=0.3,\n",
    "    stratify=y,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "Cs = np.logspace(-2, 2, 8)\n",
    "\n",
    "logreg_l1 = LogisticRegressionCV(\n",
    "    penalty=\"l1\",\n",
    "    solver=\"saga\",\n",
    "    Cs=Cs,\n",
    "    cv=3,\n",
    "    scoring=\"roc_auc\",\n",
    "    class_weight=\"balanced\",\n",
    "    max_iter=5000,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Fitting L1-regularized model...\")\n",
    "logreg_l1.fit(X_train, y_train)\n",
    "\n",
    "print(\"Chosen C:\", logreg_l1.C_[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9be89c6",
   "metadata": {},
   "source": [
    "##### Validation Performance\n",
    "\n",
    "On the validation set, the L1 model achieves: Accuracy: 0.803, ROC AUC: 0.881\n",
    "\n",
    "The AUC is nearly identical to the baseline model’s AUC (~0.88), indicating that the model’s ability to discriminate between survivors and non-survivors is preserved despite aggressive coefficient shrinkage. However, overall accuracy decreases compared to the baseline because the L1 model is now more willing to predict the minority class (death), which increases false positives but improves sensitivity to high-risk cases. This behavior is aligned with the goal of identifying patients at elevated mortality risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8ffae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, roc_curve\n",
    "y_val_proba_reg = logreg_l1.predict_proba(X_val)[:, 1]\n",
    "y_val_pred_reg  = logreg_l1.predict(X_val)\n",
    "\n",
    "#Print accuracy and AUC\n",
    "accuracy = accuracy_score(y_val, y_val_pred_reg)\n",
    "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "auc_val = roc_auc_score(y_val, y_val_proba_reg)\n",
    "print(f\"Validation AUC for regularized model: {auc_val:.4f}\")\n",
    "\n",
    "#plot the AUC curve\n",
    "fpr, tpr, _ = roc_curve(y_val, y_val_proba_reg)\n",
    "auc_score = roc_auc_score(y_val, y_val_proba_reg)\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {auc_score:.3f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - L1 LogisticRegressionCV\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# plot confusion matrix\n",
    "cm_reg = confusion_matrix(y_val, y_val_pred_reg)\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm_reg, annot=True, cmap=\"Blues\", fmt=\"d\",\n",
    "            xticklabels=[\"Pred 0\", \"Pred 1\"],\n",
    "            yticklabels=[\"True 0\", \"True 1\"])\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix - L1 Logistic Regression\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abce23f",
   "metadata": {},
   "source": [
    "Then we want to compare this regularized logistic regression with unregularized logistic regression. So we also perform unregulaized logistic regresion on the processed/imputed X data after cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775f1688",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_unreg = LogisticRegression(\n",
    "    penalty=None,\n",
    "    solver=\"lbfgs\",\n",
    "    max_iter=5000,  #use the same number of iterations as before\n",
    "    class_weight=\"balanced\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Fitting unregularized model...\")\n",
    "lr_unreg.fit(X_train, y_train)\n",
    "\n",
    "y_val_proba_unreg = lr_unreg.predict_proba(X_val)[:, 1]\n",
    "y_val_pred_unreg  = lr_unreg.predict(X_val)\n",
    "\n",
    "#Print accuracy and AUC\n",
    "acc_unreg = accuracy_score(y_val, y_val_pred_unreg)\n",
    "print(f\"Unregularized Validation Accuracy: {acc_unreg:.4f}\")\n",
    "auc_unreg = roc_auc_score(y_val, y_val_proba_unreg)\n",
    "print(f\"Unregularized Validation AUC: {auc_unreg:.4f}\")\n",
    "\n",
    "#plot the AUC curve\n",
    "fpr_unreg, tpr_unreg, _ = roc_curve(y_val, y_val_proba_unreg)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(fpr_unreg, tpr_unreg, label=f\"Unregularized AUC = {auc_unreg:.3f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve - Unregularized Logistic Regression\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "#plot the confusion matrix\n",
    "cm_unreg = confusion_matrix(y_val, y_val_pred_unreg)\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(\n",
    "    cm_unreg,\n",
    "    annot=True,\n",
    "    cmap=\"Blues\",\n",
    "    fmt=\"d\",\n",
    "    xticklabels=[\"Pred 0\", \"Pred 1\"],\n",
    "    yticklabels=[\"True 0\", \"True 1\"]\n",
    ")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix - Unregularized Logistic Regression\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f2db51",
   "metadata": {},
   "source": [
    "Using both processed/imputed data after cutoof, The L1-regularized logistic regression actually achieved almost the same, a slightly lower accurancy score and AUC score than the unregularized model.\n",
    "This behavior is consistent with theory: L1 regularization increases model sparsity by shrinking weaker coefficients to zero, which reduces model variance and improves interpretability at the cost of a small reduction in predictive performance.\n",
    "Given the high-dimensional feature space and potential multicollinearity, we view the regularized model as a more robust and parsimonious alternative, despite the modest AUC decrease.”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a9b72f",
   "metadata": {},
   "source": [
    "### 6.3 Ridge-regularized logistic regression (L2 / R^2 penalty)\n",
    "For another regularization option, we fit a ridge (L2) logistic regression that predicts `hospital_death` using the same preprocessed predictors.\n",
    "The squared L2 penalty discourages large weights, shrinking correlated coefficients together so that we stabilize estimates without forcing sparsity.\n",
    "\n",
    "#### Model Architecture\n",
    "We again rely on multiple logistic regression:\n",
    "\n",
    "$$\n",
    "\\ln\\left( \\frac{P(Y = 1 \\mid X)}{1 - P(Y = 1 \\mid X)} \\right)\n",
    "= \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p .\n",
    "$$\n",
    "\n",
    "with linear predictor\n",
    "\n",
    "$$\n",
    "z = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p ,\n",
    "$$\n",
    "\n",
    "and probability\n",
    "\n",
    "$$\n",
    "P(Y = 1 \\mid X) = \\sigma(z) = \\frac{1}{1 + e^{-z}} .\n",
    "$$\n",
    "\n",
    "The L2-penalized objective (ridge logistic regression) augments the negative log-likelihood with the squared coefficient norm:\n",
    "\n",
    "$$\n",
    "\\min_{\\beta} \\left[\n",
    "    -\\sum_{i=1}^{n} \\Big( y_i \\log p_i + (1 - y_i)\\log(1 - p_i) \\Big)\n",
    "    + \\lambda \\sum_{j=1}^{p} \\beta_j^{2}\n",
    "\\right],\n",
    "$$\n",
    "\n",
    "where $\\lambda$ controls the amount of shrinkage. Larger $\\lambda$ values push correlated coefficients toward one another, limiting variance while preserving all predictors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e360ffaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (precision_score, recall_score, average_precision_score, precision_recall_curve)\n",
    "Cs_l2 = np.logspace(-3, 2, 10)\n",
    "logreg_l2 = LogisticRegressionCV(\n",
    "    penalty=\"l2\",\n",
    "    solver=\"lbfgs\",\n",
    "    Cs=Cs_l2,\n",
    "    cv=5,\n",
    "    scoring=\"roc_auc\",\n",
    "    class_weight=\"balanced\",\n",
    "    max_iter=5000,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Fitting L2-regularized logistic regression (ridge)...\")\n",
    "logreg_l2.fit(X_train, y_train)\n",
    "print(\"Chosen C:\", logreg_l2.C_[0])\n",
    "\n",
    "y_val_proba_l2 = logreg_l2.predict_proba(X_val)[:, 1]\n",
    "y_val_pred_l2 = logreg_l2.predict(X_val)\n",
    "\n",
    "ridge_metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_val, y_val_pred_l2),\n",
    "    \"Precision\": precision_score(y_val, y_val_pred_l2, zero_division=0),\n",
    "    \"Recall\": recall_score(y_val, y_val_pred_l2),\n",
    "    \"F1\": f1_score(y_val, y_val_pred_l2),\n",
    "    \"ROC AUC\": roc_auc_score(y_val, y_val_proba_l2),\n",
    "    \"PR AUC\": average_precision_score(y_val, y_val_proba_l2)\n",
    "}\n",
    "\n",
    "print(\"Validation metrics:\")\n",
    "for metric, value in ridge_metrics.items():\n",
    "    print(f\"  {metric}: {value:.3f}\")\n",
    "\n",
    "print(\"Classification report (ridge):\")\n",
    "print(classification_report(y_val, y_val_pred_l2, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb73396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_l2 = confusion_matrix(y_val, y_val_pred_l2)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(\n",
    "    cm_l2,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=[\"Pred 0\", \"Pred 1\"],\n",
    "    yticklabels=[\"True 0\", \"True 1\"]\n",
    ")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix - Ridge Logistic Regression\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b8ac34",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_l2, tpr_l2, _ = roc_curve(y_val, y_val_proba_l2)\n",
    "precision_l2, recall_l2, _ = precision_recall_curve(y_val, y_val_proba_l2)\n",
    "baseline_val = y_val.mean()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].plot(fpr_l2, tpr_l2, label=f\"AUC = {roc_auc_score(y_val, y_val_proba_l2):.3f}\")\n",
    "axes[0].plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "axes[0].set_xlabel(\"False Positive Rate\")\n",
    "axes[0].set_ylabel(\"True Positive Rate\")\n",
    "axes[0].set_title(\"Ridge ROC Curve\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(recall_l2, precision_l2, label=f\"AP = {average_precision_score(y_val, y_val_proba_l2):.3f}\")\n",
    "axes[1].hlines(baseline_val, 0, 1, linestyle=\"--\", color=\"gray\", label=\"Prevalence\")\n",
    "axes[1].set_xlabel(\"Recall\")\n",
    "axes[1].set_ylabel(\"Precision\")\n",
    "axes[1].set_title(\"Ridge Precision-Recall Curve\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2583ca",
   "metadata": {},
   "source": [
    "#### Validation results\n",
    "\n",
    "Ridge logistic regression tuned with five-fold cross-validation selected ``C = 0.0464``. On the 27,514-patient validation split (25,139 survivors vs. 2,375 deaths) it produced the following summary metrics:\n",
    "\n",
    "| Metric | Value |\n",
    "| --- | --- |\n",
    "| Accuracy | 0.803 |\n",
    "| Precision | 0.275 |\n",
    "| Recall | 0.781 |\n",
    "| F1 | 0.407 |\n",
    "| ROC AUC | 0.881 |\n",
    "| PR AUC | 0.480 |\n",
    "\n",
    "Class-wise scores align with the confusion matrix: class 0 (survivors) precision 0.975 / recall 0.805, class 1 (deaths) precision 0.275 / recall 0.781."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b7aa1e",
   "metadata": {},
   "source": [
    "\n",
    "The ridge model maintains ROC and PR AUC values on par with the earlier L1 and unregularized fits, showing that the L2 penalty preserves ranking power while yielding smoother score distributions on the imbalanced validation split.\n",
    "Its precision and recall remain balanced because shrinkage tempers overly large coefficients without discarding predictors, so we keep broad clinical coverage and avoid the instability that can appear in a purely sparse solution.\n",
    "This behavior is consistent with the calibration plot above, where ridge probabilities follow the diagonal closely thanks to the moderated odds ratios.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0893e03b",
   "metadata": {},
   "source": [
    "### 6.4 Elastic-net logistic regression\n",
    "Elastic-net regularization combines the sparsity of L1 with the grouped shrinkage of L2, enabling us to adaptively balance feature selection and coefficient stability when modeling `hospital_death`.\n",
    "\n",
    "#### Model Architecture\n",
    "We reuse the same multiple logistic regression form:\n",
    "\n",
    "$$\n",
    "\\ln\\left( \\frac{P(Y = 1 \\mid X)}{1 - P(Y = 1 \\mid X)} \\right) = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p ,\n",
    "$$\n",
    "\n",
    "with linear predictor $z = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p$ and probability $P(Y = 1 \\mid X) = \\sigma(z) = 1/(1 + e^{-z})$.\n",
    "\n",
    "The elastic-net objective augments the negative log-likelihood with a convex combination of L1 and L2 penalties:\n",
    "\n",
    "$$\n",
    "\\min_{\\beta} \\left[ -\\sum_{i=1}^{n} \\Big( y_i \\log p_i + (1 - y_i)\\log(1 - p_i) \\Big) + \\lambda \\left(\\alpha \\sum_{j=1}^{p} |\\beta_j| + \\frac{1 - \\alpha}{2} \\sum_{j=1}^{p} \\beta_j^{2}\\right) \\right],\n",
    "$$\n",
    "\n",
    "where $\\alpha \\in [0, 1]$ controls the mix between L1 and L2 penalties. Smaller $\\alpha$ values behave more like ridge, while larger values resemble LASSO. Jointly tuning $\\alpha$ and the overall penalty strength $\\lambda$ lets us explore a continuum of regularized logistic models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70ab95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "enet_param_grid = {\n",
    "    \"C\": np.logspace(-3, 1, 6),\n",
    "    \"l1_ratio\": [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "}\n",
    "\n",
    "elastic_net = LogisticRegression(\n",
    "    penalty=\"elasticnet\",\n",
    "    solver=\"saga\",\n",
    "    class_weight=\"balanced\",\n",
    "    max_iter=5000,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "grid_enet = GridSearchCV(\n",
    "    estimator=elastic_net,\n",
    "    param_grid=enet_param_grid,\n",
    "    scoring=\"roc_auc\",\n",
    "    cv=3,\n",
    "    n_jobs=-1,\n",
    "    refit=True\n",
    ")\n",
    "\n",
    "print(\"Tuning elastic-net logistic regression...\")\n",
    "grid_enet.fit(X_train, y_train)\n",
    "print(\"Best params:\", grid_enet.best_params_)\n",
    "\n",
    "logreg_enet = grid_enet.best_estimator_\n",
    "y_val_proba_enet = logreg_enet.predict_proba(X_val)[:, 1]\n",
    "y_val_pred_enet = logreg_enet.predict(X_val)\n",
    "\n",
    "elastic_metrics = {\n",
    "    \"Accuracy\": accuracy_score(y_val, y_val_pred_enet),\n",
    "    \"Precision\": precision_score(y_val, y_val_pred_enet, zero_division=0),\n",
    "    \"Recall\": recall_score(y_val, y_val_pred_enet),\n",
    "    \"F1\": f1_score(y_val, y_val_pred_enet),\n",
    "    \"ROC AUC\": roc_auc_score(y_val, y_val_proba_enet),\n",
    "    \"PR AUC\": average_precision_score(y_val, y_val_proba_enet)\n",
    "}\n",
    "\n",
    "print(\"Validation metrics (elastic net):\")\n",
    "for metric, value in elastic_metrics.items():\n",
    "    print(f\"  {metric}: {value:.3f}\")\n",
    "\n",
    "print(\"Classification report (elastic net):\")\n",
    "print(classification_report(y_val, y_val_pred_enet, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7c5ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_enet = confusion_matrix(y_val, y_val_pred_enet)\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.heatmap(\n",
    "    cm_enet,\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=[\"Pred 0\", \"Pred 1\"],\n",
    "    yticklabels=[\"True 0\", \"True 1\"]\n",
    ")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix - Elastic Net Logistic Regression\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b507543",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "fpr_enet, tpr_enet, _ = roc_curve(y_val, y_val_proba_enet)\n",
    "precision_enet, recall_enet, _ = precision_recall_curve(y_val, y_val_proba_enet)\n",
    "baseline_val = y_val.mean()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].plot(fpr_enet, tpr_enet, label=f\"AUC = {roc_auc_score(y_val, y_val_proba_enet):.3f}\")\n",
    "axes[0].plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")\n",
    "axes[0].set_xlabel(\"False Positive Rate\")\n",
    "axes[0].set_ylabel(\"True Positive Rate\")\n",
    "axes[0].set_title(\"Elastic Net ROC Curve\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(recall_enet, precision_enet, label=f\"AP = {average_precision_score(y_val, y_val_proba_enet):.3f}\")\n",
    "axes[1].hlines(baseline_val, 0, 1, linestyle=\"--\", color=\"gray\", label=\"Prevalence\")\n",
    "axes[1].set_xlabel(\"Recall\")\n",
    "axes[1].set_ylabel(\"Precision\")\n",
    "axes[1].set_title(\"Elastic Net Precision-Recall Curve\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f397de50",
   "metadata": {},
   "source": [
    "#### Validation results\n",
    "\n",
    "Elastic-net tuning (grid search over ``C`` and ``l1_ratio``) chose ``C = 0.0398`` and ``l1_ratio = 0.3``. On the same validation set it achieved:\n",
    "\n",
    "| Metric | Value |\n",
    "| --- | --- |\n",
    "| Accuracy | 0.803 |\n",
    "| Precision | 0.275 |\n",
    "| Recall | 0.782 |\n",
    "| F1 | 0.407 |\n",
    "| ROC AUC | 0.881 |\n",
    "| PR AUC | 0.480 |\n",
    "\n",
    "Per-class results mirror the ridge model: survivors precision 0.975 / recall 0.805, deaths precision 0.275 / recall 0.782, confirming comparable discrimination with a slightly higher positive recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a154d932",
   "metadata": {},
   "source": [
    "Elastic-net achieves nearly identical ROC and PR AUC as the ridge and L1 models, indicating that the mixed penalty still discriminates well while coping with the minority class.\n",
    "By tuning the $l_1$ ratio we obtain precision and recall that sit between the sparse LASSO solution and the dense ridge solution, highlighting the most influential vitals but keeping correlated measurements grouped.\n",
    "The resulting partial sparsity and smooth shrinkage explain the calibrated probabilities observed above and make this model a flexible alternative when we want interpretability similar to the previous approaches.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d33afdc",
   "metadata": {},
   "source": [
    "# 7. Final Model Pipeline\n",
    "\n",
    "Our final model extends the baseline logistic regression by incorporating a __stacked ensemble architecture__ that integrates three complementary classifiers: logistic regression, decision tree, and random forest. Each of these base models captures different structural patterns in the data. Logistic regression models linear relationships and yields well-calibrated probabilities; decision trees capture nonlinear interactions and hierarchical splits; and random forests provide a robust, variance-reduced ensemble of trees capable of modeling complex feature interactions. By combining these three diverse learners, we aim to leverage their complementary strengths and mitigate the weaknesses of any single model.\n",
    "\n",
    "To construct the stacked ensemble, we will first train all three base learners on the training data using cross-validated out-of-fold predictions. These out-of-fold predictions ensure that the meta-learner receives unbiased estimates of each model’s predictive behavior. We then use these predictions—typically the class probabilities rather than class labels—as inputs to a meta-level logistic regression model. This top-layer logistic regression learns how to optimally weight and combine the predictions from the three base models. Because it is regularized (L1/L2/elastic net), it provides both interpretability and protection against overfitting, especially given the additional features introduced by stacking.\n",
    "\n",
    "The final prediction pipeline works in two stages: (1) a new observation is passed through the logistic regression, decision tree, and random forest base learners to obtain their predicted class probabilities; (2) these probabilities are fed into the meta-learner logistic regression, which outputs the final probability of class membership. This stacked framework retains the interpretability and simplicity of logistic regression at the top level while benefiting from the nonlinear representational power of tree-based models below. The result is a more flexible and accurate classifier that generalizes better than the baseline logistic regression alone.\n",
    "\n",
    "## 7.1 Base Learners\n",
    "\n",
    "### 7.1.1 Decision Trees\n",
    "We implemented a decision tree model to see if the model has a better performance.\n",
    "### 7.1.2 Random Forest\n",
    "We implemented a random forest model as it can decrease the risk of overfitting by reducing the variance of the trees. We tried a variety of hyperparameters such as the number of estimators and the maximum depth of the trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e887765e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
    "from sklearn.metrics import (classification_report, confusion_matrix, \n",
    "                              roc_auc_score, average_precision_score, f1_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_processed, y, \n",
    "    test_size=0.2, \n",
    "    stratify=y, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "dt_param_dist = {\n",
    "    'max_depth': [5, 10, 15, 20, None],\n",
    "    'min_samples_split': [2, 5, 10, 20],\n",
    "    'min_samples_leaf': [1, 2, 4, 8],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "dt_random_search = RandomizedSearchCV(\n",
    "    estimator=dt,\n",
    "    param_distributions=dt_param_dist,\n",
    "    n_iter=20,\n",
    "    scoring='f1_macro',\n",
    "    cv=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "dt_random_search.fit(X_processed, y)\n",
    "\n",
    "dt_best = dt_random_search.best_estimator_\n",
    "print(\"\\nBest Decision Tree Parameters:\")\n",
    "print(dt_random_search.best_params_)\n",
    "\n",
    "dt_cv_scores = cross_val_score(dt_best, X_processed, y, cv=5, scoring='f1_macro', n_jobs=-1)\n",
    "print(f\"\\nCross-validation F1-Macro: {dt_cv_scores.mean():.4f} (+/- {dt_cv_scores.std():.4f})\")\n",
    "\n",
    "y_val_pred_dt = dt_best.predict(X_val)\n",
    "y_val_proba_dt = dt_best.predict_proba(X_val)[:, 1]\n",
    "\n",
    "dt_roc_auc = roc_auc_score(y_val, y_val_proba_dt)\n",
    "dt_pr_auc = average_precision_score(y_val, y_val_proba_dt)\n",
    "dt_f1 = f1_score(y_val, y_val_pred_dt)\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Decision Tree Performance on Validation Set:\")\n",
    "print(\"-\"*80)\n",
    "print(f\"ROC AUC:  {dt_roc_auc:.4f}\")\n",
    "print(f\"PR AUC:   {dt_pr_auc:.4f}\")\n",
    "print(f\"F1 Score: {dt_f1:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_val_pred_dt))\n",
    "\n",
    "rf_cm = confusion_matrix(y_val, y_val_pred_rf)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(rf_cm)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(dt_cm, annot=True, fmt='d', cmap='Purples', \n",
    "            xticklabels=['Survived', 'Died'],\n",
    "            yticklabels=['Survived', 'Died'])\n",
    "plt.title(f'Decision Tree Confusion Matrix\\nROC AUC: {dt_roc_auc:.3f}')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "feature_names = X_train.columns if hasattr(X_train, 'columns') else [f'Feature_{i}' for i in range(X_train.shape[1])]\n",
    "\n",
    "dt_feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': dt_best.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 Most Important Features (Decision Tree):\")\n",
    "print(dt_feature_importance.head(15).to_string(index=False))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_20_dt = dt_feature_importance.head(20).sort_values('importance')\n",
    "plt.barh(top_20_dt['feature'], top_20_dt['importance'], color='mediumpurple')\n",
    "plt.xlabel('Feature Importance', fontsize=12)\n",
    "plt.title('Top 20 Feature Importances - Decision Tree', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb652a5f-84b9-4a29-bcc5-c554cbdee34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [200, 300, 500],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', None],\n",
    "    'max_samples': [0.5, 0.7, 0.9, None]\n",
    "}\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=30,\n",
    "    scoring='f1_macro',\n",
    "    cv=cv,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "rf_pipeline = random_search.best_estimator_\n",
    "print(\"Best Params:\", random_search.best_params_)\n",
    "\n",
    "cv_scores = cross_val_score(rf_pipeline, X_train, y_train, cv=cv, scoring='f1_macro', n_jobs=-1)\n",
    "print(\"Mean CV f1_macro:\", cv_scores.mean())\n",
    "\n",
    "y_val_pred = rf_pipeline.predict(X_val)\n",
    "print(\"\\nClassification report:\\n\", classification_report(y_val, y_val_pred))\n",
    "\n",
    "cm = confusion_matrix(y_val, y_val_pred)\n",
    "print(\"\\nConfusion Matrix:\\n\", cm)\n",
    "\n",
    "cm_df = pd.DataFrame(cm, index=model.classes_, columns=model.classes_)\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual')\n",
    "plt.xlabel('Predicted')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
